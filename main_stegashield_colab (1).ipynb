{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daPLDfUXpXoX"
      },
      "source": [
        "# StegaShield: Colab Main Notebook\n",
        "A fast, reproducible end-to-end pipeline with speed fixes for Colab."
      ],
      "id": "daPLDfUXpXoX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VhUf5-EpXoa"
      },
      "source": [
        "## 1) Mount Google Drive"
      ],
      "id": "9VhUf5-EpXoa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrYO5MUtpXob",
        "outputId": "c72e7336-177a-4957-c257-c9779ebc0010"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted Drive.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Mounted Drive.\")\n"
      ],
      "id": "YrYO5MUtpXob"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM2a6iYypXod"
      },
      "source": [
        "## 2) Configure paths\n",
        "Set your project root on Drive and where to sync locally for speed."
      ],
      "id": "vM2a6iYypXod"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yup2VFwwpXoe",
        "outputId": "db211367-e783-451b-dddb-140c6c176280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DRIVE_ROOT    : /content/drive/MyDrive/project_codes/models_new\n",
            "DRIVE_DATASET : /content/drive/MyDrive/project_codes/models_new/dataset/originals\n",
            "DRIVE_JPEGS   : /content/drive/MyDrive/project_codes/models_new/JpegImages\n",
            "LOCAL_DATASET : /content/dataset/originals\n",
            "LOCAL_JPEGS   : /content/JpegImages\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# === EDIT THIS IF NEEDED ===\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/project_codes/models_new\"\n",
        "\n",
        "# Derived paths\n",
        "DRIVE_DATASET = f\"{DRIVE_ROOT}/dataset/originals\"                   # pool of natural images\n",
        "DRIVE_JPEGS = f\"{DRIVE_ROOT}/JpegImages\"                  # generated splits (by generate_dataset.py)\n",
        "LOCAL_ROOT = \"/content\"                                   # local workspace\n",
        "LOCAL_DATASET = f\"{LOCAL_ROOT}/dataset/originals\"                   # local copy for speed\n",
        "LOCAL_JPEGS = f\"{LOCAL_ROOT}/JpegImages\"                  # local generated splits\n",
        "\n",
        "print(\"DRIVE_ROOT    :\", DRIVE_ROOT)\n",
        "print(\"DRIVE_DATASET :\", DRIVE_DATASET)\n",
        "print(\"DRIVE_JPEGS   :\", DRIVE_JPEGS)\n",
        "print(\"LOCAL_DATASET :\", LOCAL_DATASET)\n",
        "print(\"LOCAL_JPEGS   :\", LOCAL_JPEGS)\n"
      ],
      "id": "Yup2VFwwpXoe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqWI_5WepXof"
      },
      "source": [
        "## 3) Install dependencies"
      ],
      "id": "XqWI_5WepXof"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "air-f8m-pXog",
        "outputId": "9bbb777c-f265-4757-f877-60f9dbd5020a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dependencies installed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip -q install timm reedsolo pywavelets scikit-image joblib tqdm matplotlib\n",
        "print(\"✅ Dependencies installed.\")\n"
      ],
      "id": "air-f8m-pXog"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqWHrgL1pXoi"
      },
      "source": [
        "## 4) Navigate to project folder on Drive"
      ],
      "id": "VqWHrgL1pXoi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-CyX9XApXoj",
        "outputId": "8d074003-6498-40ff-f449-2db70ad7d6e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/project_codes/models_new\n",
            "total 127K\n",
            "-rw------- 1 root root  11K Nov  8 21:00 attacker.py\n",
            "-rw------- 1 root root  16K Nov  8 21:00 cnn_train.py\n",
            "drwx------ 2 root root 4.0K Nov  7 20:35 dataset\n",
            "-rw------- 1 root root 7.3K Nov  8 21:00 embedder.py\n",
            "-rw------- 1 root root  13K Nov  9 07:12 generate_dataset.py\n",
            "-rw------- 1 root root 2.3K Nov  9 07:12 hybrid_train.py\n",
            "-rw------- 1 root root 9.0K Nov  8 21:00 label_checker.py\n",
            "-rw------- 1 root root  16K Nov  9 08:07 main_stegashield_colab.ipynb\n",
            "drwx------ 2 root root 4.0K Nov  7 20:18 older\n",
            "drwx------ 2 root root 4.0K Nov  9 08:00 train_residual_6k\n",
            "-rw------- 1 root root 1.2K Nov  8 21:00 utils.py\n",
            "-rw------- 1 root root 3.6K Nov  8 21:00 verifier.py\n",
            "-rw------- 1 root root  35K Nov  9 07:54 watermark_core.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%cd \"{DRIVE_ROOT}\"\n",
        "!ls -lah\n"
      ],
      "id": "4-CyX9XApXoj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwWdqWOdpXok"
      },
      "source": [
        "## 5) Sanity checks for dataset pool on Drive"
      ],
      "id": "OwWdqWOdpXok"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIaU9I17pXok",
        "outputId": "e298572d-5963-4c42-9408-4885c2e74357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing top-level of DRIVE_DATASET:\n",
            "total 3.5G\n",
            "-rw------- 1 root root 143K Nov  2 17:45 2007_000027.jpg\n",
            "-rw------- 1 root root  54K Nov  2 17:45 2007_000032.jpg\n",
            "-rw------- 1 root root  70K Nov  2 17:45 2007_000033.jpg\n",
            "-rw------- 1 root root  64K Nov  2 17:45 2007_000039.jpg\n",
            "-rw------- 1 root root  81K Nov  2 17:45 2007_000042.jpg\n",
            "-rw------- 1 root root  72K Nov  2 17:45 2007_000061.jpg\n",
            "-rw------- 1 root root 124K Nov  2 17:45 2007_000063.jpg\n",
            "-rw------- 1 root root  99K Nov  2 17:45 2007_000068.jpg\n",
            "-rw------- 1 root root  79K Nov  2 17:45 2007_000121.jpg\n",
            "-rw------- 1 root root  78K Nov  2 17:45 2007_000123.jpg\n",
            "-rw------- 1 root root 104K Nov  2 17:45 2007_000129.jpg\n",
            "-rw------- 1 root root  77K Nov  2 17:45 2007_000170.jpg\n",
            "-rw------- 1 root root 131K Nov  2 17:45 2007_000175.jpg\n",
            "-rw------- 1 root root  88K Nov  2 17:45 2007_000187.jpg\n",
            "-rw------- 1 root root  81K Nov  2 17:45 2007_000241.jpg\n",
            "-rw------- 1 root root  23K Nov  2 17:45 2007_000243.jpg\n",
            "-rw------- 1 root root  68K Nov  2 17:45 2007_000250.jpg\n",
            "-rw------- 1 root root  66K Nov  2 17:45 2007_000256.jpg\n",
            "-rw------- 1 root root  54K Nov  2 17:45 2007_000272.jpg\n",
            "-rw------- 1 root root  66K Nov  2 17:45 2007_000323.jpg\n",
            "-rw------- 1 root root 135K Nov  2 17:45 2007_000332.jpg\n",
            "-rw------- 1 root root 124K Nov  2 17:45 2007_000333.jpg\n",
            "-rw------- 1 root root  83K Nov  2 17:45 2007_000346.jpg\n",
            "-rw------- 1 root root  53K Nov  2 17:45 2007_000363.jpg\n",
            "-rw------- 1 root root 111K Nov  2 17:45 2007_000364.jpg\n",
            "-rw------- 1 root root  85K Nov  2 17:45 2007_000392.jpg\n",
            "-rw------- 1 root root  89K Nov  2 17:45 2007_000423.jpg\n",
            "-rw------- 1 root root  64K Nov  2 17:45 2007_000452.jpg\n",
            "-rw------- 1 root root  98K Nov  2 17:45 2007_000464.jpg\n",
            "-rw------- 1 root root  75K Nov  2 17:45 2007_000480.jpg\n",
            "-rw------- 1 root root  73K Nov  2 17:45 2007_000491.jpg\n",
            "-rw------- 1 root root 136K Nov  2 17:45 2007_000504.jpg\n",
            "-rw------- 1 root root  96K Nov  2 17:45 2007_000515.jpg\n",
            "-rw------- 1 root root 129K Nov  2 17:45 2007_000528.jpg\n",
            "-rw------- 1 root root  55K Nov  2 17:45 2007_000529.jpg\n",
            "-rw------- 1 root root  73K Nov  2 17:45 2007_000549.jpg\n",
            "-rw------- 1 root root  74K Nov  2 17:45 2007_000559.jpg\n",
            "-rw------- 1 root root  97K Nov  2 17:45 2007_000572.jpg\n",
            "-rw------- 1 root root  73K Nov  2 17:45 2007_000584.jpg\n",
            "\n",
            "Counting images (jpg/jpeg/png) recursively:\n",
            "33260\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os, subprocess, shlex\n",
        "\n",
        "print(\"Listing top-level of DRIVE_DATASET:\")\n",
        "!ls -lah \"$DRIVE_DATASET\" | head -n 40 || echo \"⚠️ Could not list DRIVE_DATASET\"\n",
        "\n",
        "print(\"\\nCounting images (jpg/jpeg/png) recursively:\")\n",
        "!find \"$DRIVE_DATASET\" -type f \\\n",
        "  \\( -iname \"*.jpg\" -o -iname \"*.jpeg\" -o -iname \"*.png\" \\) | wc -l\n"
      ],
      "id": "VIaU9I17pXok"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOHMRZ0hpXol"
      },
      "source": [
        "## 6) Copy dataset to local runtime for speed"
      ],
      "id": "hOHMRZ0hpXol"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7mdc8iNpXol",
        "outputId": "900f2b78-2d9c-47c0-c60f-669885b6e1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          3.75G 100%    7.36MB/s    0:08:05 (xfr#33260, to-chk=0/33261)\n",
            "✅ Copied dataset to local.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Copy only once per session; rsync shows progress\n",
        "!rm -rf \"$LOCAL_DATASET\"\n",
        "!mkdir -p \"$LOCAL_DATASET\"\n",
        "!rsync -ah --info=progress2 \"$DRIVE_DATASET/\" \"$LOCAL_DATASET/\"\n",
        "print(\"✅ Copied dataset to local.\")\n"
      ],
      "id": "S7mdc8iNpXol"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OozsOI9ZpXom"
      },
      "source": [
        "## 7) Apply speed patches (lower image size & JPEG probability)"
      ],
      "id": "OozsOI9ZpXom"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOQDCkI2pXom",
        "outputId": "406cdf25-3bc6-4c2e-87cf-f257178f4561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Applied speed patches in watermark_core.py\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Reduce SimpleImageFolder(image_size=256) -> 224\n",
        "!sed -i 's/image_size=256/image_size=224/' watermark_core.py\n",
        "\n",
        "# Lower non-differentiable JPEG probability p_jpeg=0.5 -> 0.2\n",
        "!sed -i 's/def __init__(self, p_jpeg=0.5):/def __init__(self, p_jpeg=0.2):/' watermark_core.py\n",
        "\n",
        "print(\"✅ Applied speed patches in watermark_core.py\")\n"
      ],
      "id": "hOQDCkI2pXom"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvP3pYO3pXom"
      },
      "source": [
        "## 8) (Optional) Cap steps per epoch for quick runs\n",
        "*Set `MAX_STEPS` via environment; default 400 if not set.*"
      ],
      "id": "BvP3pYO3pXom"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrbgmEGBpXon",
        "outputId": "b7b2d1fe-b7d4-4fe8-a5d7-7ebf128700ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ℹ️ MAX_STEPS guard already present; skipping.\n"
          ]
        }
      ],
      "source": [
        "import os, re\n",
        "\n",
        "with open(\"watermark_core.py\", \"r\", encoding=\"utf-8\") as f:\n",
        "    src = f.read()\n",
        "\n",
        "# Properly escaped version of the check\n",
        "if 'MAX_STEPS = int(os.environ.get(\"MAX_STEPS\"' not in src:\n",
        "    src = src.replace(\n",
        "        'pbar = tqdm(dl, desc=f\"Epoch {epoch}/{epochs}\")',\n",
        "        'pbar = tqdm(dl, desc=f\"Epoch {epoch}/{epochs}\")\\n        MAX_STEPS = int(os.environ.get(\"MAX_STEPS\", \"400\"))'\n",
        "    )\n",
        "    src = src.replace(\n",
        "        'for batch_idx, imgs in enumerate(pbar):',\n",
        "        'for batch_idx, imgs in enumerate(pbar):\\n            if batch_idx >= MAX_STEPS:\\n                break'\n",
        "    )\n",
        "    with open(\"watermark_core.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(src)\n",
        "    print(\"✅ Inserted MAX_STEPS guard.\")\n",
        "else:\n",
        "    print(\"ℹ️ MAX_STEPS guard already present; skipping.\")\n"
      ],
      "id": "TrbgmEGBpXon"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwRUnMPepXon"
      },
      "source": [
        "## 9) Keep AMP on legacy API to avoid device_type kwarg issues"
      ],
      "id": "YwRUnMPepXon"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkBLnbcrpXon",
        "outputId": "192efa76-3b79-43b5-9fd3-384820152b76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ AMP configured for Colab compatibility (expect a deprecation warning; it is harmless).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ensure we use torch.cuda.amp (widely supported on Colab) to avoid TypeError\n",
        "!sed -i 's/from torch.amp import autocast, GradScaler/from torch.cuda.amp import autocast, GradScaler/' watermark_core.py\n",
        "!sed -i 's/GradScaler(device_type=\"cuda\")/GradScaler()/g' watermark_core.py\n",
        "!sed -i 's/autocast(device_type=\"cuda\", enabled=use_amp)/autocast(enabled=use_amp)/g' watermark_core.py\n",
        "print(\"✅ AMP configured for Colab compatibility (expect a deprecation warning; it is harmless).\")\n"
      ],
      "id": "ZkBLnbcrpXon"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DtyUbH2pXoo"
      },
      "source": [
        "## 10) Create a smaller residual training subset (6k images)"
      ],
      "id": "4DtyUbH2pXoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_5mCSv3pXoo",
        "outputId": "78278c7f-ae97-414b-8031-a013fd3ca91e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "find: paths must precede expression: ` '\n",
            "shuf: ' ': No such file or directory\n",
            "Count in /content/train_residual_6k:\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!rm -rf /content/train_residual_6k\n",
        "!mkdir -p /content/train_residual_6k\n",
        "!find \"$LOCAL_DATASET\" -type f \\\n",
        "  \\( -iname \"*.jpg\" -o -iname \"*.jpeg\" -o -iname \"*.png\" \\) \\  | shuf -n 6000 \\  | xargs -I{{}} cp \"{{}}\" /content/train_residual_6k/\n",
        "\n",
        "print(\"Count in /content/train_residual_6k:\")\n",
        "!find /content/train_residual_6k -type f \\\n",
        "  \\( -iname \"*.jpg\" -o -iname \"*.jpeg\" -o -iname \"*.png\" \\) | wc -l\n"
      ],
      "id": "E_5mCSv3pXoo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsmbfSsnpXoo"
      },
      "source": [
        "## 11) Train the residual encoder/decoder (quick run)"
      ],
      "id": "wsmbfSsnpXoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Iz3jPenpXop",
        "outputId": "affdbff9-5c22-489b-b4fb-21d1b7866103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100% 528M/528M [00:02<00:00, 239MB/s]\n",
            "Starting U-Net training...\n",
            "Source images: /content/train_residual_6k\n",
            "Epochs: 3, Batch Size: 32, LR: 0.0001\n",
            "Payload bits: 112\n",
            "Model will be saved to: best_residual_hybrid.pt\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/hybrid_train.py\", line 49, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/hybrid_train.py\", line 23, in main\n",
            "    core.train_residual_encoder(\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/watermark_core.py\", line 636, in train_residual_encoder\n",
            "    dl = DataLoader(\n",
            "         ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 388, in __init__\n",
            "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\", line 156, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "os.environ[\"MAX_STEPS\"] = \"400\"   # ~cap steps/epoch\n",
        "!python hybrid_train.py   --image_dir /content/train_residual_6k   --epochs 3   --batch_size 32   --num_workers 0\n"
      ],
      "id": "-Iz3jPenpXop"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcDrMK8qpXop"
      },
      "source": [
        "## 12) Configure dataset generation to 18k (balanced 3-way splits)"
      ],
      "id": "zcDrMK8qpXop"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7IhTaB0pXop",
        "outputId": "6c7f5caf-a9ad-4906-e4c9-9f9596c8d604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated CONFIG['per_split'] to balanced 18k.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Edit generate_dataset.py counts to:\n",
        "# train: 3k each; val: 1.5k each; test: 1.5k each  -> total 18k\n",
        "import io, re\n",
        "\n",
        "with open(\"generate_dataset.py\", \"r\", encoding=\"utf-8\") as f:\n",
        "    src = f.read()\n",
        "\n",
        "# Replace the 'per_split' dict robustly\n",
        "new_block = \"\"\"    'per_split': {\n",
        "        'train': {'watermarked': 3000, 'tampered': 3000, 'unwatermarked': 3000},\n",
        "        'val':   {'watermarked': 1500, 'tampered': 1500, 'unwatermarked': 1500},\n",
        "        'test':  {'watermarked': 1500, 'tampered': 1500, 'unwatermarked': 1500}\n",
        "    },\"\"\"\n",
        "\n",
        "src = re.sub(r\"'per_split':\\s*\\{[\\s\\S]*?\\},\", new_block, src, count=1)\n",
        "\n",
        "with open(\"generate_dataset.py\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(src)\n",
        "\n",
        "print(\"✅ Updated CONFIG['per_split'] to balanced 18k.\")\n"
      ],
      "id": "i7IhTaB0pXop"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUrN6cIbpXop"
      },
      "source": [
        "## 13) Generate dataset (locally)"
      ],
      "id": "LUrN6cIbpXop"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QT7qkkSPpXoq",
        "outputId": "87a060c8-be87-42bf-8454-fcfc25e0ccc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/drive/MyDrive/project_codes/models_new/generate_dataset.py\", line 46\n",
            "    'attack_presets': [\n",
            "IndentationError: unexpected indent\n",
            "ls: cannot access 'JpegImages': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Ensure originals_dir points to local dataset if you want local generation speed\n",
        "# Otherwise leave as-is to use DRIVE_DATASET.\n",
        "# Here we run with defaults; generate_dataset.py reads from CONFIG.\n",
        "!python generate_dataset.py --jobs 2\n",
        "!ls -lah JpegImages | head -n 40\n"
      ],
      "id": "QT7qkkSPpXoq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mypPA8D1pXoq"
      },
      "source": [
        "## 14) (Optional) Mirror JpegImages to local for training speed"
      ],
      "id": "mypPA8D1pXoq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fHct094pXoq",
        "outputId": "daf310c2-bbc2-40b5-8b57-0cf680d736c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rsync: [sender] change_dir \"/content/drive/MyDrive/project_codes/models_new/JpegImages\" failed: No such file or directory (2)\n",
            "\r              0 100%    0.00kB/s    0:00:00 (xfr#0, to-chk=0/0)\n",
            "rsync error: some files/attrs were not transferred (see previous errors) (code 23) at main.c(1338) [sender=3.2.7]\n",
            "✅ Local JpegImages mirror ready: True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# If JpegImages were created under DRIVE_ROOT, sync them locally for speed.\n",
        "if os.path.exists(\"JpegImages\"):\n",
        "    # JpegImages exists in current directory; copy to LOCAL_JPEGS\n",
        "    !rm -rf \"$LOCAL_JPEGS\"\n",
        "    !mkdir -p \"$LOCAL_JPEGS\"\n",
        "    !rsync -ah --info=progress2 \"JpegImages/\" \"$LOCAL_JPEGS/\"\n",
        "else:\n",
        "    # Fall back to Drive path\n",
        "    !rm -rf \"$LOCAL_JPEGS\"\n",
        "    !mkdir -p \"$LOCAL_JPEGS\"\n",
        "    !rsync -ah --info=progress2 \"$DRIVE_JPEGS/\" \"$LOCAL_JPEGS/\"\n",
        "print(\"✅ Local JpegImages mirror ready:\", os.path.exists(LOCAL_JPEGS))\n"
      ],
      "id": "8fHct094pXoq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpBPj9OGpXoq"
      },
      "source": [
        "## 15) Run label checker (auto-fix embedding issues)"
      ],
      "id": "IpBPj9OGpXoq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K69MI1OkpXoq",
        "outputId": "cf1c1350-3eff-474b-97cd-f09067a6d882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/label_checker.py\", line 5, in <module>\n",
            "    from verifier import extract_and_verify\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/verifier.py\", line 14, in <module>\n",
            "    from embedder import REGISTRY, bytes_to_tensor\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/embedder.py\", line 129, in <module>\n",
            "    from verifier import external_extract\n",
            "ImportError: cannot import name 'external_extract' from partially initialized module 'verifier' (most likely due to a circular import) (/content/drive/MyDrive/project_codes/models_new/verifier.py)\n",
            "ls: cannot access 'JpegImages/problematic': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python label_checker.py\n",
        "!ls -lah JpegImages/problematic | head -n 40 || echo \"No problematic files folder.\"\n"
      ],
      "id": "K69MI1OkpXoq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxhAaW8TpXor"
      },
      "source": [
        "## 16) Train the CNN classifier (Xception + aux features)"
      ],
      "id": "TxhAaW8TpXor"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MG5TS_OpXor",
        "outputId": "45bfc195-0c74-4379-8d6b-66731117ae98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/cnn_train.py\", line 369, in <module>\n",
            "    train(\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/cnn_train.py\", line 170, in train\n",
            "    full_ds_train = StegaDataset(metadata_csv, transform=train_transform, use_aux=use_aux, is_training=True)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/project_codes/models_new/cnn_train.py\", line 27, in __init__\n",
            "    self.df = pd.read_csv(metadata_csv).fillna(0) # Fill NaNs with 0\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
            "    return _read(filepath_or_buffer, kwds)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
            "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
            "    self._engine = self._make_engine(f, self.engine)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
            "    self.handles = get_handle(\n",
            "                   ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\", line 873, in get_handle\n",
            "    handle = open(\n",
            "             ^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'JpegImages/metadata.csv'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python cnn_train.py   --metadata JpegImages/metadata.csv   --epochs 5   --batch_size 32   --num_workers 2\n"
      ],
      "id": "8MG5TS_OpXor"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1-Zj-wopXor"
      },
      "source": [
        "## 17) Quick evaluation (example extract & verify)"
      ],
      "id": "o1-Zj-wopXor"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1vIGG5kpXos",
        "outputId": "9e264c0e-5d60-41bf-a3d0-9693e7e81c29"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'external_extract' from partially initialized module 'verifier' (most likely due to a circular import) (/content/drive/MyDrive/project_codes/models_new/verifier.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-307393260.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mverifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_and_verify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Try to pick one sample from test/tampered if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/project_codes/models_new/verifier.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Import the model registry from embedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# This is safe as embedder.py no longer imports verifier.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0membedder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mREGISTRY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_to_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexternal_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_img\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_original_img\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/project_codes/models_new/embedder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# --- This function is now provided by verifier.py ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# We keep the import here for the embed_image check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mverifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexternal_extract\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0membed_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'external_extract' from partially initialized module 'verifier' (most likely due to a circular import) (/content/drive/MyDrive/project_codes/models_new/verifier.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "\n",
        "from verifier import extract_and_verify\n",
        "from pathlib import Path\n",
        "\n",
        "# Try to pick one sample from test/tampered if available\n",
        "cand = None\n",
        "test_tamp = Path(\"JpegImages/test/tampered\")\n",
        "if test_tamp.exists():\n",
        "    for p in test_tamp.glob(\"*\"):\n",
        "        if p.suffix.lower() in [\".jpg\",\".jpeg\",\".png\"]:\n",
        "            cand = str(p)\n",
        "            break\n",
        "\n",
        "if cand:\n",
        "    # Heuristic way to infer original path (your pipeline stores it in metadata too)\n",
        "    stem = Path(cand).stem\n",
        "    # This is an example — adjust if your naming differs\n",
        "    original_guess = next(Path(\"dataset/originals\").glob(f\"{stem.split('_')[0]}*\"), None)\n",
        "    print(\"Sample Tampered:\", cand)\n",
        "    print(\"Original Guess :\", str(original_guess) if original_guess else \"not found\")\n",
        "    if original_guess:\n",
        "        res = extract_and_verify(cand, str(original_guess), params={'payload_bytes': b'StegaShield_v1', 'digest_bits': 128})\n",
        "        print(res)\n",
        "else:\n",
        "    print(\"No sample found in JpegImages/test/tampered to demo extract_and_verify.\")\n"
      ],
      "id": "-1vIGG5kpXos"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBv5VN63pXos"
      },
      "source": [
        "## 18) (Optional) Save key artifacts back to Drive"
      ],
      "id": "aBv5VN63pXos"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGjuH-pypXos"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Save trained models/checkpoints back to Drive for persistence\n",
        "!mkdir -p \"$DRIVE_ROOT/checkpoints\"\n",
        "!cp -f best_residual_hybrid.pt \"$DRIVE_ROOT/checkpoints/\" 2>/dev/null || true\n",
        "!cp -f stegashield_cnn_final.pth \"$DRIVE_ROOT/checkpoints/\" 2>/dev/null || true\n",
        "print(\"✅ Saved checkpoints to\", f\"{DRIVE_ROOT}/checkpoints\")\n"
      ],
      "id": "LGjuH-pypXos"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYAJ5OhNpXot"
      },
      "source": [
        "## 19) Session info"
      ],
      "id": "tYAJ5OhNpXot"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhcyj5tApXot"
      },
      "outputs": [],
      "source": [
        "\n",
        "!nvidia-smi\n",
        "import torch, platform\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Python:\", platform.python_version())\n"
      ],
      "id": "mhcyj5tApXot"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}